#<-------- training related -------->#
gpu: True
learning_rate: 0.0004 # 0.0004 pyyaml has issues with scientific notation
num_epochs: 2400
sav_per_num_iter: 1000  # set sav_per_num_epochs=-1 when used sav_per_num_iter
sav_per_num_epochs: -1  # set sav_per_num_iter=-1 when used sav_per_num_epochs
tot_batch_size: 200 #160 #160 # 200 # total batch size for train_mproc
tot_val_batch_size: 40 #64 #64 #40 #120  # total val batch size for train_mproc
inference_batch_size: 200
set_ddp: True  #
frame_num: 300 # 500ms  600
use_warmup: True

total_iter_kmeans: 300
max_grad_norm: 1.0 # 0.25
disable_adaptive_grad_norm: true
seed: null
tags: null
minlen: null
maxlen: null
log_step: 100
save_dir: "debug/ckpt_models" # where to save the model checkpoint
#dataset: 'lmdblibri' # lmdb890/lmdb150/raw890/rawlibri/TIMIT/lmdb890-label(mask label)

## new optimizer config
new_opt: false
optimizer:
  opt_G:
    name: adam
    betas: [0.9,0.999] # adam betas
    lr: 0.0003  # GAN: 0.00003 (generator pretrained)
    G_warmup_time: 0
  opt_D:
    name: adam
    betas: [0.9,0.999]
    weight_decay:  0.000001
    lr: 0.0003 #0.0003

#<----GAN related---->#
#generator_pretrained: true
use_adv_loss: false
disc_gt_aug: false
disc_input_mfcc: false
load_old_gan_models: false
use_disc_each_bitrate: false
load_disc_para: false
adversarial_loss: 'MSE'  #hinge/MSE/wasserstein/BCE
clip_value: 0.01
n_disc: 1      # at 1kbps/3kbps/6kbps, set n_disc=1, n_generator=1
n_generator: 2    # at 0.256kbps/0.512kbps, set n_disc=1, n_generator=10
D_warmup_time: 0
freeze_gen: False
use_disc_feat_loss: True
disc_feat_loss_weight: 0.1
adv_loss_weight: 0.001 #0.001
use_condition_input: False
use_freq_disc: true
use_time_disc: False
use_spk_disc: false
freq_disc_type: 'v6'
d_freq_weight: 1
g_freq_weight: 1
time_disc_type: 'v1'
d_time_weight: 1
g_time_weight: 1
spk_disc_type: 'v3'
d_spk_weight: 1
g_spk_weight: 1

### soundstream's STFTDiscriminator
use_freq_sd_disc: False
D_sd_loss_weight: 1.0
G_sd_loss_weight: 0.01
adv_sd_fm_loss_weight: 0.1
### soundstream's WaveDiscriminator
use_time_sd_disc: False


load_encoder_para: True
load_decoder_para: True
load_spk_codebook: True
load_phn_codebook: True

#<-------- network structure related -------->#
model_type: "tfnet_v2i_vqvae_disentangle"  #tfnet_v2i_vqvae/tfnet_v0i_vqvae_scalable/tfnet_v2i_vqvae_timedomain/tfnet_v2i_vqvae_scalable
model_type_main: "tfnetv0_48khz_25ms_interleave_groupGRU2_2"
model_type_refine: "tfnetv0_48khz_25ms_interleave_groupGRU2_2"
# "tfnetv4_interleave_groupGRU2"
# "tfnetv4_interleave_groupTSA"
# "tfnetv0_interleave"
# "tfnetv0_interleave_groupGRU2"
# "tfnetv0_interleave_groupTSA_noncausal"

# "tfnetv2_interleave_vqvae"
# "tfnetv4_interleave_vqvae"
# "tfnetv4_interleave_gTSA_vqvae"
# "tfnetv4_interleave_vqvae_48khz_scalable"
# 'tfnetv2_interleave_groupGRU2_large_vqvae'
# 'tfnetv2_interleave_groupGRU2_large2_vqvae'
# 'tfnetv0_interleave_groupGRU2_vqvae'

# "tfnetv2_interleave_multiraterps_vqvae"  # for representation learning with multi-bitrate VQVAE
# "wavLM_vqvae"   
# "tfnet_v2i_vqvae_disentangle"
# "tfnet_v2i_vqvae_disentangle_mbart"
# "tfnet_v2i_vqvae_disentangle_s2s"

# "tfnetv2_interleave_vqvae_withpre"
# "tfnetv2_interleave_vqvae_twodec"
# "tfnetv2_interleave_vqvae_controldns_twodec"
# "tfnetv2_interleave_vqvae_withprepost"

# "tfnetv0_48khz_25ms_interleave_groupGRU2"
# "tfnetv0_48khz_25ms_interleave_groupGRU2_2"
# "tfnetv0_48khz_25ms_interleave_groupTSA"
# "tfnetv0_48khz_25ms_interleave_groupTSA_2"
# "tfnetv0_48khz_20ms_interleave_groupGRU2"
# "tfnetv0_48khz_twostage_dns"

# 'tfnetv2_tcm_vqvae'
# 'tfnetv2_gru_vqvae'
# 'tfnetv2_groupGRU2_vqvae'
# 'tfnetv2_interleave_gru_vqvae'
# 'tfnetv2_interleave_groupGRU2_vqvae'
# 'tfnetv2_interleave_groupTSA2_vqvae'
# 'tfnetv2_interleave_groupTSA_vqvae'
# 'tfnetv2_interleave_TSA_vqvae'
# 'tfnetv2_tcm_vqvae_symm'
# 'tfnetv2_gru_vqvae_symm'
# 'tfnetv2_interleave_gru_vqvae_symm'
# 'tfnetv2_interleave_groupGRU2_vqvae_symm'
# 'tfnetv2_interleave_groupTSA2_vqvae_symm'
# 'tfnetv2_interleave_groupTSA_vqvae_symm'
# 'tfnetv2_interleave_TSA_vqvae_symm'

# 'tfnetv2_tcm_vqvae_symm_twodec'
# 'tfnetv2_gru_vqvae_symm_twodec'
# 'tfnetv2_interleave_gru_vqvae_symm_twodec'
# 'tfnetv2_interleave_groupGRU2_vqvae_symm_twodec'
# 'tfnetv2_interleave_groupTSA2_vqvae_symm_twodec'
# 'tfnetv2_interleave_groupTSA_vqvae_symm_twodec'
# 'tfnetv2_interleave_TSA_vqvae_symm_twodec'
bn: True
activation: PRELU # ELU/PRELU
use_encoder_tcm: True
use_encoder_gru: True
use_timedomain_output: False
pre_model_type: 'v4' # for "tfnetv2_interl_codec_4d_lowga_withpre"
tcm_prelu_fix: True
use_scale_float16: False
use_online_feature_norm: False
use_less_dilations: False
use_complete_latent: False  # not used in disentangle (fixed)

use_learnable_compression: False # disable so that disentangle decoder can use 1kbps
use_learnable_feat_cprs: False
input_cprs_power: 0.3

use_random_sampled_decoder: False
random_sampling_type: 'uniform' # 'uniform', 'entropy'

#use_learned_reshape_bottleneck: True

#<---predictive-------------->#
use_predictive: False
prediction_type: 'conv-two' # 'conv', 'gru', 'adaptive', 'conv-two'
use_learnable_alpha: False  # used only when prediction_type is 'adaptive'
use_affine_adaptive: True  # used only when prediction_type is 'adaptive'
prediction_stage: '3' # '1': teacher forcing using corrupted uncompressed features, '2': use real compressed features (propagation through time), '3': teacher forcing using last model output
fuse_type: 'conv' # 'res', 'conv'
use_BPTT: True  # used only when prediction_stage is '2'
use_context_detach: False # detach context for reconstruction loss, only BP for prediction loss
use_sparse_loss: False
sparse_loss_weight: 0.005
use_predictive_loss: True
use_detached_pred_loss: True  # True
pred_loss_weight: 0.02 #0.25 0.01
use_compressed_channels: True
use_tanh_before_vq: False
use_tanh_features: False


#<---autoregressive-------------->#
autoregressive: False #
autoregressive_merge: 'transformer' # 'concat' or 'transformer'
autoregressive_stage: '1' # '1': teacher forcing, '2': use last model output

#<-------- vq related -------->#
bitrate: '0.256k'  #0.256k/0.512k/1k/3k/6k/12k/all4
bitrates_selected: ['0.256k','0.512k','1k','3k'] # ['3k','6k','9k','12k']
bitrate_enh_layer: '1k'
6k_type: '2' # 1/2  different vq types
#use_EMA: True
decay: 0.99 # used for EMA
combineVQ_frames: 4
#use_subcodebook_residue: True # combineVQ_frames must be 1 when it is true
vq_rate: 'redundant' # 'complete', 'redundant'
vq_type: 'Gumbel' # 'EMA', 'Gumbel'
vq_initial: true   # set to false if loading codebook from model, default true, for EMA only
use_parallelvq: false # for both predictive and non-predictive coding
num_parallel_vq: 16  # set to > 1 to save memory for bitrate > 6k
use_vq_loss: False   # set to False for Gumbel, commitment loss
vq_loss_weight: 0.01 #0.25

#<----Gumbel related---->#
groups: 1
combine_groups: False
temperature: [2.0, 0.5, 0.9999995]  # max(2*0.999999^iter, 0.5)
dist_to_logits_alpha: -5 # -5, -1
#<----entropy related---->#
use_entropy_loss: true
entropy_loss_type: '2'
entropy_loss_weight: 0.04 #0.04 # 0.01 # 0.04
entropy_fuzz: 4800 # bit/frame
use_defined_gumbelsoftmax: True  # True

#<-------- representation related -------->#
pretrained_path: None #.\WavLM\WavLM-Base+.pt
use_weighted_rep: True
dvector_path: ./d-vector
use_dvector: False

#<-------- disentangle related -------->#
disen_scheme: 'global_spk' # 'global_spk': use global spk feature  'rate1': spk/phn bitrate = 1/3
merge_type: 'multicondition1'  #'concat'/'condition'/'multicondition1'/'multicondition2'
disable_disen: False
disen_type: 'ins' # 'grl'/'ins'
spk_grl: False
phn_grl: False
use_InsNorm_enc: True
use_InsNorm_dec: False
use_InsNorm_input: False
disable_spk_vq: True
disable_phn_vq: False
auto_rate_allocation: False
use_noncausal_spk: False
use_temporal_aggregator: False # causal spk embedding
tap_rate: 10
max_pooling_len: 10 #-1 # -1: global causal pooling
transmit_rate: 10
train_downstream_model_noVQ: False
disen_model_type: 'org'  # 'org', 'mAdaIN', 'v1', 'v2' # default 'v1'
use_xuemodel_bias: True
use_channel_attention: False
use_causalIN: False
use_groupTSA: False


# supervision related
suprv_position: 'vq_in'
disable_classify: False # todo
disable_phn_classify: True
disable_spk_classify: True
spk_loss_weight1: 0.1
spk_loss_weight2: 0.01
phn_loss_weight1: 0.05
phn_loss_weight2: 0.01

# data augmentation related
data_aug: False
do_peq: True
do_formant: True
do_pitch: True
use_contrastive_loss: False
neg_num: 10
contrastive_loss_weight: 10

# dns supervision related
use_spk_pearson_dns: False
spk_peason_loss_weight: 0.1
use_gt_rec_dns: False

use_KD_dns: False
use_phn_pearson_dns: False
phn_peason_loss_weight: 0.1 
use_codebook_gt_dns: True

use_2dec_dns: False


#<-------- multistage related -------->#
multistage: False
stage: stage3
decoder_initial: False
freeze_decoder: False
freeze_encoder: False
freeze_content_encoder: False
freeze_codebook: False
freeze_spk_codebook: False
freeze_phn_codebook: False
train_best_quality : False


#<-------- scalable model related -------->#
base_codec_path: 1.ckpt
freeze_base_codec: False

#<-------- PLC related -------->#
add_packet_loss: false
plc_unit_type: 'none' # 'full', 'low', 'none'  # 'none' means no concealment capability for codec part (only set received features as zero)

load_sender_only: false
tune_g_s_only: false
freeze_pre_dns: false
freeze_codec: false

#<-------- DNS related -------->#
#add_implicit_dns: true
input_data_idx: 1   # 0: noisy, 1: clean
target_data_idx: 1
mask_data_idx: 2  # for PLC training with useZeroInPLC=False
cleanonly_data: false
control_encoder_only: false
control_decoder_only: false
freeze_main_dns: false  # for two-stage dns model training

use_pretrained_auxinput: False
pretrained_feat_type: 'vq_in' # 'vq_out', 'vq_in'

use_speaker_separation: False
use_singlespeaker_disturb: False

# "tfnetv0_interl"/"tfnetv0_interl_enc_merge"/"tfnetv0_interl_tcm_merge"/'tfnetv2'/'tfnetv2_interl'/'tfnetv4'/
# "tfnetv6"/"tfnetv6_enc_merge"/"tfnetv6_tcm_merge"/"tfnetv0_tsa"
tcm_merge_type: 'concat'  # 'concat' or 'transformer'. Used when "tcm_merge" in model_type.

#<-------- input and target representation related -------->#
f_min: 20    # 20 for 16khz, 30 for 24khz, 60 for 48khz
f_max: 8000 # 8000 for 16khz, 12000 for 24khz, 24000 for 48khz

# sampling_rate: 16000
# hop_vqvae: 0.25
# hop_fraction: 0.5
# dft_size: 320  #20ms/10ms 
# frame_dur: 0.02
# vq_in_dur: 0.005

sampling_rate: 16000
hop_vqvae: 0.25
hop_fraction: 0.5
dft_size: 640   #40ms/10ms
frame_dur: 0.04
vq_in_dur: 0.01

# sampling_rate: 48000
# hop_vqvae: 0.25
# hop_fraction: 0.5
# dft_size: 1920   #40ms/10ms/48khz
# frame_dur: 0.04

# sampling_rate: 16000
# hop_vqvae: 0.5
# hop_fraction: 0.5
# dft_size: 640   #40ms/20ms 
# frame_dur: 0.04

# sampling_rate: 16000
# hop_vqvae: 0.2
# hop_fraction: 0.4
# dft_size: 400   #25ms/5ms
# frame_dur: 0.025

# sampling_rate: 24000 
# hop_vqvae: 0.25
# hop_fraction: 0.5 
# dft_size: 960   #40ms/10ms/24khz
# frame_dur: 0.04

# sampling_rate: 48000 
# hop_vqvae: 0.4 
# hop_fraction: 0.4  # 0.4 for non-AR, 0.25 for AR
# dft_size: 1200    #25ms/10ms/48khz
# frame_dur: 0.025
 
# sampling_rate: 48000 
# hop_vqvae: 0.5 
# hop_fraction: 0.5 
# dft_size: 960   #20ms/10ms/48khz
# frame_dur: 0.02

feature: "stft" # "stft"
feature_norm: false #'const'
disable_stft_out_mask: False
use_compressed_input: True
#learn_uncompressed_gain: False
learn_uncompressed_amp: False
use_complex_gain: False

use_pre_filter_inbandCprs: True
use_post_filter_inbandCprs: True

#<-------- loss function related -------->#
use_bin_loss: True
loss_type: "power-compressed-mse" # Options: "power-compressed-mse"
weight_amplitude: 0.5  # default is 0.5
power: 0.3
bin_loss_weight: 1.0
feat_loss_weight: 0.01
different_feat_loss: 'type1'
use_err_recon2: False

use_perceptual_loss: False
perceptual_loss_type: '1'
perceptual_loss_weight: 0.1  # 0.1 for dns, 1 for codec, 0.1 for codec with GAN
wav2vec_path: losses\wav2vec\wav2vec_large.pt  # debug only

use_multiscale_mel_loss: True
mel_loss_type: 'multi_window' # /multi_window/multi_band/
mel_loss_weight: 0.25 #0.25  # codec only: 0.1/0.25
use_log_L2_term: False
mel_L2_term_weight: 0.05

add_intermediate_loss: false
weight_intermediate_loss: 10.0

weight_aux_rec_loss: 1.0


use_framewise_compress: False
frame_by_frame_inference: False


#<-------- evaluation and inference related -------->#
dnsmos_model_path: "dnsmos/model_v8.onnx"
pick_model: 'v2' # 'v1'(default): rank, 'v2': rank + val recon error top5.
decmos_version: 'v2'  # 'v1' or 'v2'
test_data: 'v3'  # used for pick_model='v1'. 'v1': v1/v2/v3/icassp2021_dns_challenge/clean_nest, 'v2': dns_test_set, 'v3': v4/icassp2021_dns_challenge.
save_raw_signal: False  # set to true to save the updated lpb and mic when doing inference

#<-------- others -------->#
global_stats: null 
use_global_stats: False 
