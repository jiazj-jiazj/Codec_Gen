Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (numpy 1.24.0 (/home/v-zhijunjia/.local/lib/python3.10/site-packages), Requirement.parse('numpy!=1.19.3,<1.24; sys_platform == "linux"'), {'azureml-dataset-runtime'}).
2024-03-25 02:27:29 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
/home/v-zhijunjia/miniconda3/envs/valle-4-23/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
  warnings.warn(
Current working directory: /home/v-zhijunjia/CodecGen
only_antoregressive is True
add_prenet is False
self.ar_text_prenet : Identity()
add_prenet：False
self.encoder_layers:6
self.decoder_layers：6
only_antoregressive is True
add_prenet is False
self.ar_text_prenet : None
add_prenet：False
[]
  0%|          | 0/64 [00:00<?, ?it/s]2024-03-25 02:27:49 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:27:49 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
  2%|▏         | 1/64 [00:07<07:50,  7.48s/it]2024-03-25 02:27:55 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:27:55 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
  3%|▎         | 2/64 [00:20<11:16, 10.90s/it]2024-03-25 02:28:08 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:28:08 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
  5%|▍         | 3/64 [00:39<14:31, 14.29s/it]2024-03-25 02:28:26 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:28:26 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
processing 0th semantic_sys file
0
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: BUT WAS THAT ALL HER REWARD ONE OF THE LADIES ASKED
enroll_x_lens:tensor([43], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
176
[17, 296, 159, 159, 159, 159, 159, 167, 385, 35, 131, 133, 133, 364, 364, 276, 276, 141, 141, 141, 141, 141, 141, 141, 368, 368, 453, 9, 9, 198, 198, 45, 45, 45, 45, 45, 285, 335, 14, 411, 297, 297, 297, 297, 297, 297, 297, 293, 293, 497, 497, 58, 58, 156, 156, 156, 156, 245, 42, 42, 147, 456, 456, 456, 456, 301, 43, 364, 364, 276, 276, 153, 153, 153, 153, 153, 372, 372, 372, 396, 396, 313, 24, 24, 133, 133, 364, 364, 364, 276, 276, 174, 174, 174, 174, 174, 174, 282, 282, 282, 388, 195, 195, 195, 195, 117, 117, 404, 404, 225, 225, 225, 80, 140, 140, 140, 287, 69, 223, 223, 223, 130, 130, 402, 198, 198, 22, 22, 283, 455, 455, 251, 251, 241, 431, 431, 171, 171, 171, 252, 325, 325, 41, 41, 324, 318, 318, 368, 453, 9, 168, 168, 180, 145, 145, 486, 376, 376, 376, 376, 460, 460, 169, 169, 150, 39, 86, 86, 105, 105, 336, 82, 96, 96, 227, 419]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 174, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0036
generate
processing 1th semantic_sys file
1
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: THERE WAS A UNANIMOUS GROAN AT THIS AND MUCH REPROACH AFTER WHICH IN HIS PREOCCUPIED WAY HE EXPLAINED
enroll_x_lens:tensor([52], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
344
[17, 296, 127, 0, 222, 378, 378, 345, 141, 141, 281, 281, 9, 168, 44, 44, 44, 219, 219, 219, 219, 485, 485, 374, 374, 10, 10, 479, 331, 331, 365, 365, 365, 365, 365, 460, 330, 94, 94, 469, 469, 203, 53, 459, 459, 459, 459, 271, 31, 54, 142, 142, 221, 336, 208, 79, 79, 380, 380, 499, 84, 84, 496, 496, 274, 413, 413, 94, 199, 415, 415, 415, 415, 35, 401, 82, 127, 114, 114, 258, 258, 258, 258, 271, 271, 39, 39, 390, 390, 390, 18, 112, 112, 427, 56, 56, 56, 56, 56, 56, 47, 82, 47, 47, 47, 140, 140, 73, 73, 140, 209, 83, 55, 55, 55, 322, 67, 67, 250, 217, 217, 70, 70, 383, 383, 383, 383, 383, 35, 310, 107, 447, 447, 397, 42, 147, 147, 456, 456, 456, 456, 301, 143, 129, 259, 74, 74, 190, 190, 487, 380, 499, 499, 499, 496, 496, 496, 274, 358, 233, 233, 310, 107, 107, 447, 483, 483, 226, 82, 209, 145, 145, 145, 486, 460, 460, 460, 460, 169, 402, 402, 6, 272, 300, 300, 382, 245, 245, 43, 43, 345, 407, 407, 407, 407, 407, 407, 143, 36, 310, 107, 447, 447, 483, 226, 226, 188, 188, 340, 340, 340, 116, 33, 183, 183, 257, 257, 257, 257, 257, 31, 9, 142, 221, 336, 336, 74, 190, 190, 190, 487, 288, 485, 324, 464, 464, 180, 106, 405, 405, 405, 206, 178, 178, 35, 192, 192, 485, 485, 469, 469, 143, 129, 82, 74, 437, 437, 265, 265, 265, 85, 85, 146, 146, 24, 314, 133, 364, 276, 109, 109, 403, 403, 403, 403, 207, 207, 207, 207, 19, 19, 454, 454, 229, 229, 82, 312, 312, 187, 187, 82, 187, 187, 187, 163, 140, 140, 316, 140, 73, 140, 140, 373, 451, 451, 30, 30, 30, 464, 464, 154, 154, 154, 96, 96, 66, 232, 482, 105, 105, 336, 336, 354, 425, 386, 386, 386, 431, 290, 290, 290, 290, 290, 434, 434, 434, 339, 303, 303, 243, 243, 131, 419, 439]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 342, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0003
generate
processing 2th semantic_sys file
2
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: THE AWKWARD THING WAS THAT THEY HAD PRACTICALLY NO OTHER RELATIONS AND THAT HIS OWN AFFAIRS TOOK UP ALL HIS TIME
enroll_x_lens:tensor([44], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
474
[17, 17, 296, 127, 5, 448, 448, 448, 14, 14, 411, 411, 411, 284, 284, 284, 405, 405, 206, 178, 178, 35, 458, 458, 208, 441, 441, 109, 109, 382, 313, 313, 314, 314, 401, 401, 164, 164, 164, 164, 214, 214, 214, 214, 214, 214, 214, 214, 214, 328, 328, 328, 200, 200, 200, 117, 117, 117, 117, 117, 117, 48, 48, 417, 417, 170, 170, 170, 20, 20, 28, 28, 20, 28, 28, 20, 2, 20, 2, 2, 20, 20, 2, 20, 20, 163, 163, 163, 163, 163, 163, 163, 316, 20, 316, 20, 73, 73, 20, 320, 7, 345, 345, 141, 141, 141, 141, 141, 281, 281, 453, 9, 9, 198, 198, 127, 45, 45, 45, 45, 45, 45, 45, 167, 35, 35, 35, 198, 127, 114, 0, 0, 0, 0, 0, 0, 171, 3, 3, 3, 58, 58, 72, 110, 110, 254, 254, 254, 254, 314, 314, 129, 129, 401, 401, 74, 74, 190, 190, 190, 488, 488, 488, 488, 488, 488, 460, 178, 178, 35, 96, 35, 36, 272, 469, 469, 469, 469, 143, 458, 458, 26, 26, 359, 359, 359, 166, 166, 166, 166, 166, 324, 3, 301, 10, 309, 479, 331, 231, 231, 231, 231, 231, 88, 88, 88, 493, 493, 493, 493, 493, 493, 216, 300, 300, 300, 495, 42, 42, 147, 147, 456, 456, 456, 456, 251, 251, 251, 251, 241, 431, 431, 431, 171, 171, 171, 171, 418, 252, 252, 99, 99, 99, 436, 436, 436, 60, 60, 298, 298, 298, 275, 275, 379, 379, 471, 471, 471, 471, 49, 269, 390, 390, 18, 112, 112, 427, 56, 56, 247, 312, 312, 312, 187, 187, 12, 292, 12, 12, 1, 12, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408, 408, 408, 408, 149, 149, 228, 20, 412, 83, 83, 55, 55, 55, 55, 322, 322, 67, 466, 466, 127, 114, 45, 45, 45, 45, 45, 45, 45, 45, 240, 325, 325, 183, 183, 183, 257, 257, 257, 257, 257, 257, 368, 368, 453, 453, 168, 168, 106, 106, 350, 350, 350, 350, 350, 350, 413, 413, 94, 199, 255, 255, 255, 349, 349, 234, 234, 261, 25, 470, 470, 264, 264, 264, 264, 264, 468, 468, 468, 304, 304, 304, 304, 185, 49, 269, 54, 54, 238, 238, 6, 336, 108, 108, 295, 295, 295, 295, 295, 295, 143, 458, 458, 144, 27, 230, 230, 230, 230, 230, 230, 215, 215, 35, 259, 354, 106, 297, 297, 297, 297, 297, 297, 297, 297, 297, 293, 293, 58, 58, 183, 183, 257, 257, 257, 257, 257, 257, 31, 9, 9, 238, 6, 336, 108, 119, 119, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 85, 85, 85, 299, 299, 299, 203, 203, 381, 381, 381, 48, 48, 48]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 472, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0028
gener  8%|▊         | 5/64 [00:48<09:50, 10.01s/it]2024-03-25 02:28:35 | WARNING | phonemizer | words count mismatc  8%|▊         | 5/64 [01:00<13:50, 14.07s/it]2024-03-25 02:28:48 | WARNING | phonemizer | words count mismatch on 100.0% o  9%|▉         | 6/64 [01:01<10:52, 11.26s/it]2024-03-25 02:28:49 | WARNING | phonemizer | words count mismatc[17, 17, 296, 111, 111, 438, 143, 458, 192, 389, 389, 389, 314, 314, 133, 147, 147, 380, 499, 428, 428, 428, 146, 252, 457, 457, 457, 401, 401, 401, 401, 401, 401, 401, 20, 20, 108, 108, 119, 119, 351, 374, 374, 374, 374, 132, 399, 70, 70, 46, 46, 46, 46, 46, 399, 217, 473, 473, 136, 136, 136, 136, 136, 282, 282, 388, 94, 199, 89, 89, 322, 94, 199, 121, 121, 121, 33, 394, 76, 465, 208, 208, 425, 386, 386, 386, 431, 496, 496, 496, 496, 4[17, 17, 296, 111, 111, 438, 438, 458, 192, 389, 389, 389, 389, 314, 133, 147, 147, 380, 499, 499, 428, 428, 428, 146, 146, 457, 457, 457, 401, 75, 108, 108, 377, 377, 374, 374, 374, 374, 374, 374, 374, 132, 132, 132, 132, 132, 98, 13, 229, 247, 247, 126, 126, 326, 326, 326, 101, 101, 149, 149, 228, 20, 20, 7, 70, 70, 46, 46, 46, 46, 438, 399, 217, 217, 473, 473, 136, 136, 136, 136, 136, 136, 136, 282, 282, 388, 388, 94, 199, 89, 89, 446, 116, 94, 199, 121, 121, 121, 121, 394txt2semenroll_x_lens:tensor([51], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
198
[17, 296, 296, 111, 438, 438, 458, 144, 389, 389, 389, 314, 133, 147, 147, 499, 499, 428, 428, 146, 252, 457, 35, 401, 75, 108, 108, 119, 351, 374, 374, 374, 132, 132, 399, 70, 46, 46, 46, 46, 438, 399, 217, 473, 65, 136, 136, 136, 136, 282, 282, 388, 199, 199, 89, 89, 116, 199, 199, 121, 121, 121, 394, 76, 465, 208, 208, 386, 386, 496, 496, 496, 274, 368, 368, 9, 9, 198, 22, 283, 455, 143, 458, 75, 445, 445, 351, 213, 213, 213, 246, 246, 246, 246, 3, 183, 451, 30, 30, 30, 143, 144, 389, 389, 389, 389, 478, 478, 232, 172, 115, 273, 432, 432, 330, 64, 64, 212, 384, 180, 180, 315, 315, 450, 348, 466, 466, 22, 283, 455, 129, 321, 74, 437, 351, 486, 486, 460, 460, 178, 458, 192, 192, 277, 385, 325, 34, 253, 253, 253, 453, 9, 30, 30, 30, 422, 349, 234, 261, 25, 106, 480, 480, 480, 480, 85, 299, 299, 339, 64, 77, 49, 168, 106, 106, 428, 146, 146, 252, 143, 75, 108, 119, 351, 213, 213, 213, 213, 246, 19, 19, 454, 454, 78, 140, 140, 140, 140]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 196, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0005
generate
processing 4th semantic_sys file
4
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: IT SOUNDED DULL IT SOUNDED STRANGE AND ALL THE MORE SO BECAUSE OF HIS MAIN CONDITION WHICH WAS
enroll_x_lens:tensor([50], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
423
[17, 17, 17, 296, 287, 287, 111, 428, 428, 146, 146, 146, 358, 358, 233, 233, 233, 75, 227, 419, 419, 439, 78, 20, 170, 20, 20, 20, 312, 187, 12, 12, 12, 12, 12, 12, 12, 12, 12, 260, 260, 260, 260, 260, 260, 391, 391, 391, 20, 20, 373, 66, 66, 172, 115, 273, 273, 315, 315, 315, 315, 450, 413, 413, 64, 64, 212, 34, 191, 191, 191, 314, 314, 314, 32, 239, 384, 371, 106, 106, 106, 481, 481, 481, 182, 182, 182, 182, 182, 375, 375, 375, 375, 375, 375, 375, 98, 98, 98, 98, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 287, 287, 287, 287, 111, 111, 111, 438, 438, 438, 422, 143, 129, 401, 20, 108, 119, 119, 351, 213, 213, 213, 213, 213, 246, 246, 246, 246, 246, 246, 3, 422, 186, 162, 54, 172, 115, 273, 273, 315, 315, 315, 450, 450, 413, 64, 212, 34, 191, 191, 191, 314, 314, 314, 478, 478, 482, 482, 482, 482, 238, 6, 336, 336, 336, 161, 79, 79, 487, 288, 288, 290, 290, 290, 290, 290, 434, 434, 434, 434, 434, 434, 434, 434, 339, 339, 195, 471, 471, 310, 310, 107, 395, 395, 89, 89, 89, 446, 446, 446, 67, 212, 212, 131, 106, 106, 297, 297, 297, 297, 293, 293, 216, 216, 22, 283, 455, 399, 399, 70, 138, 138, 138, 138, 138, 138, 138, 372, 372, 372, 372, 396, 313, 186, 186, 162, 54, 172, 115, 273, 273, 84, 84, 84, 84, 84, 16, 16, 16, 16, 16, 16, 16, 375, 375, 375, 98, 98, 98, 13, 229, 20, 247, 312, 312, 126, 292, 292, 292, 292, 23, 23, 23, 23, 23, 23, 23, 101, 101, 101, 149, 228, 20, 320, 354, 420, 420, 422, 143, 458, 144, 27, 351, 151, 151, 151, 151, 240, 240, 368, 453, 9, 168, 223, 223, 130, 402, 183, 183, 257, 257, 257, 257, 281, 9, 142, 196, 196, 217, 473, 473, 290, 290, 290, 290, 290, 290, 290, 434, 434, 434, 434, 434, 434, 339, 339, 195, 195, 195, 90, 90, 76, 465, 144, 27, 121, 121, 121, 33, 33, 394, 212, 239, 371, 278, 278, 278, 278, 99, 436, 436, 60, 60, 298, 298, 298, 116, 195, 250, 250, 345, 407, 407, 407, 407, 407, 310, 107, 447, 397, 133, 364, 276, 141, 141, 346, 346, 265, 265, 265, 265, 85, 85, 85, 85, 85, 282, 282, 37, 37, 185, 185, 185, 269, 323, 323, 323, 18, 18, 18, 112, 112]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 421, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0034
generate
processing 5th semantic_sys file
5
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: THERE WERE PLENTY OF PEOPLE TO HELP BUT OF COURSE THE YOUNG LADY WHO SHOULD GO DOWN AS GOVERNESS WOULD BE IN SUPREME AUTHORITY
enroll_x_lens:tensor([33], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
338
[17, 296, 127, 114, 0, 0, 378, 378, 345, 347, 347, 347, 347, 245, 129, 129, 259, 74, 425, 425, 386, 431, 319, 319, 319, 330, 94, 41, 324, 324, 324, 464, 462, 462, 462, 402, 129, 401, 259, 74, 74, 351, 213, 213, 213, 252, 215, 129, 259, 29, 100, 100, 497, 497, 497, 122, 129, 401, 259, 108, 377, 344, 344, 374, 374, 132, 132, 58, 58, 72, 72, 110, 110, 443, 139, 139, 139, 293, 293, 215, 215, 35, 233, 419, 427, 229, 82, 247, 312, 126, 126, 292, 292, 292, 23, 23, 23, 23, 23, 101, 101, 101, 149, 228, 228, 140, 140, 320, 159, 159, 159, 285, 255, 255, 402, 402, 221, 259, 144, 208, 441, 441, 153, 153, 372, 372, 396, 186, 186, 54, 54, 86, 198, 22, 5, 5, 448, 448, 219, 219, 464, 180, 180, 319, 319, 319, 348, 200, 248, 250, 251, 241, 431, 431, 171, 171, 171, 171, 252, 325, 41, 41, 324, 324, 3, 183, 489, 489, 489, 489, 489, 186, 99, 338, 338, 395, 389, 389, 389, 314, 314, 90, 239, 144, 180, 180, 84, 496, 496, 274, 274, 236, 239, 239, 371, 180, 180, 315, 315, 315, 315, 450, 450, 413, 94, 199, 253, 253, 253, 253, 9, 142, 221, 336, 144, 180, 180, 151, 151, 173, 173, 29, 29, 396, 313, 94, 94, 199, 459, 459, 459, 271, 271, 39, 39, 390, 390, 390, 390, 18, 18, 112, 427, 56, 56, 56, 56, 56, 56, 56, 47, 47, 47, 140, 47, 140, 316, 316, 140, 73, 140, 320, 7, 345, 345, 389, 389, 389, 314, 314, 32, 239, 354, 420, 420, 420, 420, 324, 464, 340, 340, 340, 116, 394, 478, 478, 232, 172, 224, 494, 494, 494, 129, 129, 74, 190, 190, 487, 288, 288, 360, 360, 434, 434, 203, 53, 70, 255, 255, 255, 255, 349, 164, 164, 164, 106, 106, 153, 153, 153, 372, 372, 372, 467, 469, 469, 469, 325, 325, 41, 41, 41, 41, 19, 19, 19, 454, 454]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_to 14%|█▍        | 9/64 [01:37<10:32, 11.49s/it]2024-03-25 02:29:24 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:29:24 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
[296, 17, 22, 5, 455, 38, 349, 205, 261, 25, 498, 498, 498, 498, 498, 169, 186, 54, 238, 6, 272, 223, 223, 223, 130, 198, 198, 114, 124, 124, 124, 124, 124, 318, 368, 9, 221, 221, 259, 119, 119, 437, 151, 151, 151, 169, 35, 310, 107, 107, 395, 50, 50, 50, 50, 50, 185, 49, 9, 142, 221, 336, 144, 27, 121, 121, 116, 33, 394, 90, 4, 280, 470, 470, 403, 403, 403, 171, 171, 207, 246, 246, 246, 37, 24, 24, 404, 404, 229, 82, 247, 126, 126, 23, 23, 101, 149, 149, 228, 140, 140, 127, 45, 45, 45, 45, 198, 22, 5, 5, 455, 42, 147, 380, 278, 278, 278, 457, 242, 242, 242, 33, 33, 394, 478, 162, 232, 232, 482, 238, 12%|█▎        | 8/64 [01:52<16:01, 17.16s/it]2024-03-25 02:29:40 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:29:40 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
[17, 17, 127, 5, 5, 455, 349, 349, 234, 234, 261, 25, 498, 498, 498, 498, 396, 396, 186, 186, 39, 86, 86, 238, 6, 272, 69, 223, 223, 130, 198, 198, 124, 124, 124, 124, 124, 318, 31, 86, 238, 6, 108, 119, 351, 151, 151, 151, 240, 35, 35, 310, 107, 107, 50, 50, 50, 50, 49, 342, 9, 221, 221, 144, 27, 121, 121, 116, 33, 394, 4, 280, 280, 470, 403, 403, 403, 171, 207, 252, 252, 314, 314, 198, 45, 45, 45, 45, 35, 259, 22, 5, 5, 455, 42, 42, 147, 380, 288, 278, 278, 457, 457, 242, 242, 33, 33, 394, 478, 162, 232, 86, 238, 6, 272, 470, 470, 171, 171, 252, 252, 457, 457, 196, 291, 291, 291, 291, 379, 457, 457, 401, 82, 108, 108, 295, 295, 295, 295, 295, 458, 192, 180, 230, 230, 230, 215, 35, 35, 198, 22, 283, 283, 455, 236, 129, 259, 108, 119, 119, 351, 171, 171, 171, 171, 171, 464, 139, 139, 302, 375, 497, 497, 335, 335, 440, 440, 415, 415, 415, 415, 285, 44, 44, 44, 236, 129, 259, 74, 74, 441, 441, 441, 153, 153, 387, 387, 299, 299, 299, 358, 358, 243, 233, 227, 227, 419, 427, 56, 56, 247, 312, 126, 292, 292, 23, 23, 408, 408, 408, 149, 228, 20, 20, 412, 83, 145, 145, 460, 460, 460, 169, 35, 402, 35, 272, 272, 300, 382, 406, 467, 467, 111, 111, 438, 438, 422, 143, 36, 108, 108, 119, 119, 351, 213, 213, 213, 213, 213, 246, 246, 246, 246, 246, 246, 19, 19, 19, 454, 454, 414, 414, 414, 20, 47, 20, 20, 47, 20, 47, 20, 20, 20, 20, 20, 20, 20, 373, 72, 110, 110, 254, 254, 254, 254, 240, 325, 34, 340, 340, 340, 94, 199, 44, 44, 399, 217, 217, 473, 65, 136, 365, 365, 365, 330, 388, 199, 199, 382, 382, 245, 245, 8, 354, 420, 420, 420, 416, 458, 144, 180, 180, 180, 319, 319, 319, 282, 282, 282, 388, 388, 303, 303, 117, 117, 48, 414, 414]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 346, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0026
generate
processing 7th semantic_sys file
7
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: YOU'LL EASILY JUDGE WHY WHEN YOU HEAR BECAUSE THE THING HAD BEEN SUCH A SCARE HE CONTINUED TO FIX ME
enroll_x_lens:tensor([51], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
688
[17, 17, 296, 152, 139, 139, 175, 175, 81, 81, 356, 356, 356, 281, 453, 9, 168, 168, 26, 26, 359, 359, 81, 166, 166, 166, 324, 422, 422, 239, 310, 310, 107, 395, 395, 180, 180, 151, 151, 151, 151, 151, 151, 240, 240, 240, 24, 24, 310, 107, 447, 447, 397, 397, 397, 364, 364, 276, 276, 276, 346, 346, 346, 346, 346, 265, 265, 265, 265, 85, 85, 85, 85, 85, 207, 207, 207, 207, 207, 19, 454, 454, 225, 225, 225, 225, 225, 7, 7, 345, 409, 409, 409, 409, 116, 67, 219, 219, 152, 152, 152, 152, 152, 132, 132, 58, 58, 183, 183, 183, 286, 286, 286, 286, 286, 468, 468, 468, 468, 59, 59, 245, 245, 8, 8, 354, 420, 420, 420, 422, 143, 82, 144, 27, 351, 351, 151, 151, 151, 240, 368, 368, 453, 9, 9, 198, 198, 22, 5, 5, 455, 455, 38, 349, 164, 164, 164, 214, 214, 214, 214, 214, 214, 214, 328, 200, 200, 248, 58, 58, 110, 254, 254, 254, 254, 314, 314, 32, 32, 239, 354, 137, 137, 137, 137, 137, 137, 137, 282, 282, 282, 388, 195, 195, 195, 195, 117, 117, 117, 48, 417, 417, 417, 417, 417, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 218, 40, 218, 218, 218, 218, 218, 218, 218, 218, 140, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 140, 218, 218, 218, 218, 140, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 140, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 140, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 140, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 140, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 140, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 366, 366, 366, 366, 366, 366, 140, 366, 366, 366, 366, 366, 366, 366, 140, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 140, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 140, 366, 366, 366, 366, 366, 366, 366, 366, 366, 316, 316, 316, 316, 316, 316, 316, 316, 316, 316, 73, 289, 373, 373, 66, 66, 172, 172, 115, 273, 273, 278, 278, 278, 240, 236, 36, 310, 107, 107, 395, 395, 180, 329, 329, 329, 151, 151, 240, 240, 24, 24, 310, 107, 107, 447, 395, 494, 494, 38, 38, 162, 54, 482, 105, 105, 336, 144, 445, 470, 470, 264, 264, 264, 264, 264, 264, 468, 468, 468, 468, 59, 59, 59, 59, 452, 452, 452, 263, 263, 263, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 373, 373, 183, 451, 451, 30, 30, 30, 30, 422, 458, 144, 27, 121, 121, 121, 33, 394, 76, 465, 108, 119, 119, 351, 278, 278, 278, 360, 330, 339, 64, 398, 398, 398, 485, 374, 313, 314, 314, 314, 401, 82, 108, 108, 119, 351, 374, 374, 374, 374, 374, 132, 132, 349, 349, 234, 234, 234, 261, 25, 25, 470, 278, 278, 278, 178, 143, 96, 96, 270, 323, 323, 142, 221, 196, 196, 429, 429, 429, 429, 429, 19, 19, 454, 454, 454]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 686, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0013
generate
processing 8th semantic_sys file
8
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: SHE WAS THE MOST AGREEABLE WOMAN I'VE EVER KNOWN IN HER POSITION SHE WOULD HAVE BEEN WORTHY OF ANY WHATEVER
enroll_x_lens:tensor([58], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
237
[17, 296, 400, 400, 30, 301, 378, 345, 141, 141, 141, 141, 281, 9, 9, 198, 22, 283, 455, 399, 70, 65, 65, 17%|█▋        | 11/64 [02:11<13:17, 15.05s/it]2024-03-25 02:29:58 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:29:58 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
 19%|█▉        | 12/64 [02:19<11:15, 12.99s/it]2024-03-25 02:30:06 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:30:06 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
[17, 17, 287, 287, 111, 111, 438, 438, 143, 36, 108, 108, 119, 351, 444, 213, 213, 213, 246, 246, 246, 301, 378, 43, 345, 345, 141, 141, 141, 281, 453, 168, 168, 242, 116, 379, 394, 478, 478, 232, 172, 115, 273, 273, 278, 278, 203, 53, 76, 259, 74, 26, 359, 359, 474, 474, 474, 474, 324, 301, 216, 216, 127, 45, 45, 45, 45, 45, 457, 310, 310, 338, 400, 400, 400, 400, 30, 422, 422, 162, 232, 172, 115, 273, 273, 470, 120, 240, 314, 314, 478, 478, 66, 232, 172, 115, 273, 273, 84, 84, 84, 16, 16, 274, 98, 98, 13, 229, 20, 247, 312, 312, 187, 187, 12, 12, 12, 23, 23, 23, 260, 260, 391, 391, 20, 228, 289, 20, 20, 159, 159, 159, 236, 198, 127, 45, 45, 45, 45, 285, 34, 111, 111, 111, 438, 10, 10, 10, 398, 398, 398, 398, 374, 374, 422, 186, 99, 338, 400, 400, 400, 30, 30, 3, 58, 72, 72, 110, 110, 254, 254, 240, 314, 314, 242, 242, 379, 37[17, 17, 296, 287, 287, 284, 284, 265, 265, 265, 85, 85, 85, 146, 207, 207, 3, 454, 229, 414, 247, 126, 126, 101, 101, 51, 51, 228, 20, 20, 320, 345, 141, 141, 141, 281, 453, 168, 168, 242, 379, 379, 457, 478, 478, 232, 172, 115, 273, 278, 278, 203 17%|█▋        | 11/64 [02:43<16:34, 18.77s/it]2024-03-25 02:30:31 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_tokentop_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 235, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0011
generate
processing 9th semantic_sys file
9
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: IT WASN'T SIMPLY THAT SHE SAID SO BUT THAT I KNEW SHE HADN'T I WAS SURE I COULD SEE
enroll_x_lens:tensor([35], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
320
[17, 17, 296, 296, 111, 111, 111, 438, 438, 143, 35, 36, 108, 119, 351, 213, 213, 213, 301, 378, 378, 43, 345, 141, 141, 141, 141, 281, 453, 453, 168, 242, 242, 116, 33, 394, 478, 478, 68, 68, 68, 115, 273, 278, 278, 278, 53, 76, 76, 259, 26, 359, 359, 359, 474, 474, 474, 474, 474, 19, 19, 19, 19, 454, 454, 454, 225, 225, 225, 225, 80, 80, 20, 20, 20, 127, 45, 45, 45, 45, 45, 143, 310, 310, 107, 400, 400, 400, 30, 422, 422, 162, 162, 68, 68, 115, 273, 470, 120, 120, 240, 314, 314, 314, 478, 478, 68, 68, 68, 115, 273, 273, 84, 84, 84, 84, 16, 16, 16, 375, 375, 98, 98, 98, 98, 98, 13, 13, 13, 78, 20, 20, 170, 20, 20, 312, 312, 292, 23, 23, 23, 23, 101, 101, 149, 149, 228, 20, 20, 320, 320, 159, 159, 159, 159, 35, 35, 127, 127, 45, 45, 45, 45, 285, 34, 111, 111, 111, 111, 438, 438, 10, 10, 10, 398, 398, 398, 398, 398, 398, 374, 374, 132, 132, 132, 132, 186, 186, 99, 338, 338, 400, 400, 400, 400, 30, 30, 58, 58, 58, 110, 110, 110, 254, 254, 254, 254, 240, 240, 314, 242, 242, 242, 195, 195, 195, 243, 76, 36, 227, 227, 419, 483, 483, 226, 226, 226, 209, 209, 287, 111, 111, 111, 111, 438, 438, 378, 43, 43, 345, 141, 141, 141, 141, 141, 281, 186, 99, 338, 338, 338, 338, 338, 395, 395, 487, 498, 498, 498, 498, 498, 468, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 452, 452, 452, 263, 225, 226, 226, 226, 226, 20, 209, 287, 111, 111, 111, 438, 143, 143, 458, 458, 192, 389, 389, 389, 389, 314, 314, 478, 478, 68, 68, 68, 68, 115, 115, 267, 267, 267, 267, 267, 267, 246, 246, 246, 19, 19, 454, 454]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 318, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0012
generate
processing 10th semantic_sys file
10
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: IT WAS THIS OBSERVATION THAT DREW FROM DOUGLAS NOT IMMEDIATELY BUT LATER IN THE EVENING A REPLY THAT HAD THE INTERESTING CONSEQUENCE TO WHICH I CALL ATTENTION
enroll_x_lens:tensor([49], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
702
[17, 296, 296, 287, 284, 284, 428, 428, 146, 146, 252, 143, 36, 108, 449, 449, 119, 351, 41, 41, 41, 246, 246, 246, 246, 246, 19, 19, 19, 454, 454, 454, 454, 225, 78, 47, 47, 47, 140, 140, 140, 80, 80, 321, 321, 7, 7, 364, 345, 345, 141, 141, 141, 141, 281, 453, 9, 9, 198, 127, 114, 258, 258, 258, 258, 31, 54, 54, 224, 106, 106, 106, 481, 481, 215, 215, 215, 96, 368, 453, 168, 494, 494, 494, 173, 173, 280, 280, 418, 418, 418, 418, 418, 418, 418, 99, 99, 436, 60, 60, 60, 298, 298, 298, 195, 195, 195, 117, 117, 117, 117, 117, 197, 197, 197, 197, 7, 7, 127, 45, 45, 45, 45, 45, 385, 457, 32, 32, 32, 32, 239, 161, 161, 161, 79, 487, 189, 374, 374, 374, 374, 132, 132, 132, 132, 132, 132, 132, 132, 132, 98, 98, 98, 197, 197, 393, 393, 155, 155, 165, 165, 165, 165, 165, 165, 203, 53, 53, 53, 394, 212, 32, 32, 239, 384, 180, 180, 284, 319, 319, 319, 416, 416, 416, 192, 26, 359, 81, 81, 459, 459, 271, 271, 271, 39, 39, 390, 390, 390, 390, 390, 390, 390, 390, 18, 18, 97, 97, 97, 97, 97, 225, 7, 7, 309, 309, 479, 307, 307, 307, 307, 61, 61, 449, 449, 34, 121, 121, 121, 121, 217, 217, 473, 473, 485, 213, 213, 213, 252, 325, 485, 485, 485, 469, 134, 457, 457, 26, 359, 359, 474, 474, 474, 474, 474, 474, 19, 19, 19, 19, 454, 454, 454, 225, 225, 225, 225, 80, 80, 321, 320, 159, 159, 159, 159, 167, 457, 26, 251, 241, 241, 431, 431, 443, 171, 171, 171, 252, 143, 36, 449, 449, 449, 334, 334, 355, 355, 355, 355, 355, 355, 452, 452, 452, 452, 263, 263, 263, 225, 225, 225, 225, 225, 412, 188, 188, 340, 340, 340, 466, 22, 283, 448, 448, 448, 14, 14, 411, 411, 213, 213, 213, 213, 213, 213, 213, 173, 173, 280, 280, 242, 242, 116, 94, 199, 176, 176, 176, 328, 328, 328, 328, 328, 200, 200, 195, 117, 117, 117, 117, 117, 117, 117, 197, 197, 440, 440, 440, 44, 44, 44, 44, 42, 42, 147, 147, 147, 380, 288, 278, 215, 215, 129, 259, 74, 425, 425, 386, 386, 431, 265, 265, 265, 265, 265, 265, 85, 85, 85, 85, 207, 207, 207, 207, 207, 19, 19, 454, 454, 454, 78, 78, 170, 140, 140, 312, 312, 312, 312, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 260, 260, 260, 260, 260, 260, 260, 260, 391, 391, 391, 140, 140, 289, 321, 321, 127, 127, 45, 45, 45, 45, 45, 385, 457, 457, 335, 335, 58, 72, 72, 72, 72, 72, 110, 110, 110, 110, 254, 254, 254, 254, 254, 254, 254, 282, 282, 282, 282, 282, 37, 37, 37, 37, 24, 24, 24, 24, 131, 404, 404, 404, 404, 439, 439, 439, 439, 439, 78, 78, 170, 140, 28, 28, 140, 2, 2, 2, 2, 2, 140, 2, 140, 316, 316, 140, 316, 140, 316, 321, 321, 321, 7, 7, 7, 127, 127, 5, 448, 448, 448, 448, 3, 14, 14, 14, 411, 411, 121, 121, 121, 121, 121, 33, 64, 76, 310, 161, 161, 487, 487, 487, 469, 186, 31, 54, 54, 238, 238, 6, 272, 176, 176, 176, 176, 328, 328, 200, 200, 195, 248, 248, 248, 76, 465, 144, 27, 27, 370, 370, 370, 370, 370, 370, 64, 77, 77, 54, 54, 224, 494, 494, 469, 469, 458, 144, 208, 441, 11, 11, 11, 379, 379, 379, 77, 77, 54, 54, 238, 238, 6, 75, 108, 377, 377, 123, 123, 374, 374, 374, 374, 132, 132, 43, 364, 345, 407, 407, 407, 407, 407, 407, 310, 310, 107, 107, 395, 483, 483, 440, 287, 287, 111, 111, 111, 111, 438, 438, 143, 458, 144, 144, 27, 441, 481, 481, 424, 424, 182, 497, 175, 175, 81, 255, 255, 255, 236, 129, 321, 108, 119, 119, 351, 432, 432, 432, 432, 330, 379, 64, 243, 310, 436, 436, 60, 60, 298, 298, 298, 275, 303, 303, 303, 303, 303, 48]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 700, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0000
generate
processing 11th semantic_sys file
11
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize textbefore_semantic:
after is :
280
[17, 296, 127, 448, 448, 448, 464, 464, 493, 493, 493, 493, 493, 216, 216, 300, 300, 50, 50, 50, 185, 185, 269, 269, 390, 390, 390, 390, 18, 18, 112, 427, 56, 56, 20, 312, 312, 187, 187, 23, 23, 23, 23, 23, 101, 101, 149, 149, 228, 20, 20, 320, 147, 456, 456, 456, 368, 368, 453, 9, 168, 432, 432, 432, 330, 64, 76, 449, 449, 191, 191, 191, 314, 314, 401, 259, 259, 74, 437, 49[17, 1 20%|██        | 13/64 [03:04<12:01, 14.15s/it]2024-03-25 02:30:51 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:30:51 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
 22%|██▏       | 14/64 [03:12<10:18, 12.38s/it]2024-03-25 02:31:00 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
20output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/base[17, 17, 296, 296, 345, 109, 109, 443, 443, 443, 139, 175, 175, 118, 118, 118, 118, 118, 205, 261, 106, 111, 111, 111, 111, 111, 438, 438, 236, 239, 239, 384, 180, 84, 350, 350, 350, 350, 413, 457, 457, 457, 309, 309, 479, 331, 331, 84, 84, 84, 16, 16, 16, 16, 16, 274, 98, 98, 98, 98, 13, 13, 414, 414, 414, 170, 170, 170, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 362, 140, 362, 362, 362, 140, 362, 362, 140, 362, 362, 140, 362, 362, 362, 140, 362, 362, 362, 362, 362, 362, 218,output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0000
generate
processing 11th semantic_sys file
11
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: WELL IF I DON'T KNOW WHO SHE WAS IN LOVE WITH I KNOW WHO HE WAS
enroll_x_lens:tensor([43], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
293
[17, 296, 345, 109, 109, 443, 139, 175, 175, 118, 118, 118, 118, 118, 261, 25, 111, 111, 111, 111, 438, 438, 236, 239, 384, 371, 180, 84, 84, 350, 350, 274, 413, 457, 457, 196, 309, 309, 479, 331, 84, 84, 84, 84, 84, 84, 84, 84, 16, 16, 16, 274, 132, 274, 98, 98, 98, 13, 13, 13, 78, 78, 170, 140, 47, 140, 47, 140, 47, 140, 316, 73, 73, 140, 373, 373, 489, 489, 489, 374, 374, 132, 132, 132, 132, 186, 99, 338, 338, 400, 400, 400, 400, 30, 301, 301, 378, 43, 364, 276, 276, 346, 346, 346, 481, 481, 481, 481, 481, 481, 206, 206, 206, 37, 185, 185, 185, 269, 54, 54, 390, 390, 390, 390, 18, 18, 112, 112, 439, 439, 439, 78, 78, 140, 140, 47, 47, 47, 47, 140, 73, 73, 140, 412, 412, 188, 188, 340, 340, 340, 116, 33, 250, 251, 251, 241, 241, 431, 266, 266, 266, 266, 266, 173, 173, 402, 402, 133, 345, 333, 333, 220, 220, 164, 164, 106, 106, 284, 284, 265, 265, 265, 265, 265, 85, 85, 85, 146, 146, 438, 301, 10, 309, 479, 479, 331, 84, 84, 84, 84, 84, 16, 16, 16, 16, 274, 274, 98, 98, 98, 98, 13, 13, 13, 13, 78, 170, 170, 170, 28, 140, 2, 2, 140, 2, 140, 2, 2, 2, 2, 163, 140, 163, 163, 316, 316, 140, 316, 73, 73, 289, 373, 373, 489, 489, 489, 489, 374, 374, 132, 132, 132, 132, 132, 58, 58, 183, 451, 30, 30, 30, 301, 378, 378, 345, 141, 141, 141, 141, 141, 141, 141, 141, 141, 37, 37, 185, 185, 185, 185, 269, 269, 390, 390, 390, 390, 390, 18, 18, 112, 112, 439, 439]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 291, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0022
generate
processing 12th semantic_sys file
12
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: THE OTHERS RESENTED POSTPONEMENT BUT IT WAS JUST HIS SCRUPLES THAT CHARMED ME
enroll_x_lens:tensor([39], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
277
[17, 296, 5, 448, 448, 448, 464, 180, 493, 493, 493, 493, 216, 300, 300, 300, 382, 304, 304, 304, 304, 185, 185, 49, 269, 323, 323, 97, 97, 97, 197, 197, 197, 7, 7, 147, 147, 147, 456, 456, 456, 368, 453, 453, 168, 432, 432, 432, 432, 330, 64, 76, 449, 191, 191, 191, 314, 314, 314, 32, 401, 401, 82, 74, 74, 437, 437, 496, 496, 496, 274, 274, 186, 186, 323, 142, 221, 221, 336, 82, 74, 74, 437, 437, 496, 496, 350, 350, 413, 413, 348, 33, 250, 291, 291, 291, 291, 291, 379, 243, 243, 227, 419, 419, 427, 82, 82, 312, 312, 187, 187, 187, 187, 187, 187, 187, 187, 163, 163, 163, 163, 163, 163, 140, 316, 140, 73, 140, 140, 320, 7, 159, 159, 159, 159, 285, 285, 111, 111, 111, 111, 438, 438, 143, 129, 259, 108, 119, 119, 351, 213, 213, 213, 213, 246, 246, 246, 301, 378, 345, 345, 141, 141, 141, 141, 281, 281, 9, 9, 142, 238, 221, 336, 82, 310, 107, 107, 395, 395, 395, 151, 151, 169, 169, 150, 150, 86, 86, 238, 6, 272, 272, 472, 183, 183, 257, 257, 257, 257, 257, 368, 368, 162, 162, 482, 482, 482, 482, 482, 105, 105, 336, 208, 79, 79, 487, 487, 374, 374, 215, 215, 259, 354, 29, 100, 302, 302, 497, 497, 497, 497, 185, 49, 269, 9, 198, 198, 114, 45, 45, 45, 240, 236, 129, 310, 310, 107, 107, 395, 351, 499, 306, 306, 306, 306, 396, 396, 396, 203, 53, 64, 212, 131, 131, 472, 196, 196, 429, 429, 429, 429, 429, 19, 19, 454, 454, 229]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 275, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0006
generate
processing 13th semantic_sys file
13
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: SOMEONE ELSE TOLD A STORY NOT PARTICULARLY EFFECTIVE WHICH I SAW HE WAS NOT FOLLOWING
enroll_x_lens:tensor([42], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
216
[17, 17, 17, 296, 66, 68, 172, 115, 231, 231, 231, 231, 53, 250, 250, 174, 174, 174, 348, 94, 199, 145, 139, 139, 139, 139, 293, 293, 169, 186, 39, 54, 86, 238, 6, 108, 119, 351, 424, 424, 424, 424, 122, 122, 34, 44, 44, 38, 162, 54, 482, 238, 6, 272, 106, 106, 153, 153, 372, 372, 467, 337, 337, 41, 324, 339, 10, 10, 479, 331, 307, 307, 307, 61, 61, 167, 35, 401, 20, 74, 492, 492, 236, 236, 20, 108, 119, 278, 278, 278, 178, 458, 20, 192, 485, 485, 134, 134, 175, 81, 300, 300, 382, 134, 134, 175, 359, 166, 166, 166, 324, 464, 255, 255, 349, 349, 234, 261, 25, 470, 151, 151, 151, 178, 178, 96, 96, 20, 20, 272, 459, 459, 459, 459, 173, 402, 402, 402, 345, 345, 407, 407, 407, 407, 36, 310, 107, 395, 111, 111, 111, 438, 186, 162, 232, 172, 172, 273, 273, 106, 405, 405, 405, 206, 206, 58, 183, 4 28%|██▊       | 18/64 [03:31<09:19, 12.16s/it][17, 296, 208, 208, 190, 487, 499, 499, 265, 265, 265, 85, 146, 146, 24, 131, 133, 133, 364, 276, 174, 174, 174, 174, 348, 199, 199, 223, 130, 130, 402, 198, 198, 22, 283, 283, 455, 455, 43, 364, 276, 109, 109, 109, 278, 278, 203, 53, 473, 136, 275, 116, 94, 183, 183, 451, 30, 30, 422, 143, 259, 108, 295, 295, 295, 295, 295, 457, 457, 196, 196, 309, 479, 331, 231, 231, 231, 231, 231, 274, 413, 348, 10, 10, 479, 331, 331, 84, 496, 496, 496, 252, 143, 36, 449, 449, 459, 459[17, 17, 296, 208, 190, 487, 499, 499, 499, 265, 85, 85, 146, 146, 24, 314, 133, 133, 345, 174, 174, 174, 348, 199, 223, 223, 223, 216, 22, 283, 455, 455, 43, 364, 276, 109, 109, 278, 203, 53, 473, 242, 275, 116, 94, 183, 451, 451, 30, 422, 236, 36, 108, 119, 295, 295, 295, 143, 458, 96, 401, 196, 196, 479, 331, 231, 231, 231, 274, 10, 10, 10, 479, 331, 84, 496, 496, 496, 285, 285, 459, 459, 31, 342, 342, 224, 69, 69, 223, 130, 402, 402, 156, 156, 156, 156, 156, 59, 59, 59, 59, 452, 452, 263, 229, 82, 247, 312, 126, 126, 292, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 101, 408, 408, 149, 149, 140, 140, 373, 451, 451, 30, 30, 301, 378, 251, 241, 367, 367, 367, 367, 96, 96, 82, 272, 415, 415, 415, 457, 457, 217, 429, 429, 429, 429, 429, 246, 246, 301, 8, 8, 159, 159, 159, 159, 385, 457, 335, 335, 440, 145, 253, 253, 253, 253, 253, 253, 453, 342, 342, 168, 494, 118, 118, 402, 402, 121, 121, 1 25%|██▌       | 16/64 [03:43<09:51, 12.32s/it]2024-03-25 02:31:31 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:31:31 | WAR[17, 296, 296, 190, 380, 499, 499, 265, 265, 265, 85, 85, 146, 146, 24 27%|██▋       | 17/64 [03:54<09:31, 12.15s/it]2024-03-25 02:31:42 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:31:42 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
[17, 17, 296, 108, 377, 377, 374, 374, 374, 374, 374, 132, 132, 132, 216, 216, 127, 114, 114, 258, 258, 258, 258, 258, 258, 271, 271, 39, 39, 54, 390, 390, 390, 390, 18, 97, 97, 225, 225, 225, 183, 183, 451, 257, 257, 257, 257, 453, 453, 9, 168, 483, 14, 411, 145, 145, 365, 365, 365, 330, 330, 379, 77, 77, 77, 54, 224, 224, 300, 334, 334, 382, 245, 245, 43, 345, 345, 141, 141, 141, 281, 281, 142, 142, 221, 336, 82, 74, 190, 190, 488, 488, 488, 426, 426, 426, 426, 203, 215, 76, 35, 96, 36, 227, 419, 419, 439, 439, 439, 78, 78, 170, 170, 20, 28, 28, 20, 28,output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0007
generate
processing 15th semantic_sys file
15
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: CRIED ONE OF THE WOMEN HE TOOK NO NOTICE OF HER HE LOOKED AT ME BUT AS IF INSTEAD OF ME HE SAW WHAT HE SPOKE OF
enroll_x_lens:tensor([31], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
407
[17, 296, 296, 208, 208, 487, 499, 499, 499, 499, 265, 265, 85, 146, 146, 146, 314, 133, 133, 364, 276, 276, 174, 174, 174, 174, 174, 94, 199, 223, 223, 223, 216, 216, 283, 283, 455, 43, 43, 276, 109, 109, 109, 278, 278, 203, 53, 473, 136, 136, 275, 275, 116, 33, 404, 183, 183, 451, 30, 30, 30, 422, 143, 259, 108, 108, 119, 295, 295, 295, 295, 295, 35, 35, 96, 196, 196, 309, 479, 331, 231, 231, 231, 231, 231, 274, 274, 274, 413, 10, 10, 479, 331, 331, 84, 84, 496, 496, 496, 274, 285, 34, 459, 459, 459, 271, 31, 54, 54, 224, 224, 69, 69, 223, 223, 130, 402, 58, 72, 72, 72, 156, 156, 156, 498, 498, 498, 498, 59, 59, 59, 59, 59, 59, 59, 452, 263, 263, 414, 414, 414, 312, 312, 312, 187, 187, 187, 187, 12, 12, 12, 12, 12, 12, 12, 12, 12, 260, 260, 260, 260, 391, 391, 140, 140, 140, 140, 373, 451, 451, 30, 30, 30, 30, 301, 301, 251, 251, 251, 251, 251, 241, 367, 367, 367, 367, 367, 367, 367, 35, 96, 35, 272, 34, 415, 415, 415, 415, 415, 457, 457, 196, 196, 217, 473, 429, 429, 429, 429, 429, 246, 19, 19, 19, 454, 454, 414, 414, 82, 312, 312, 187, 187, 187, 187, 187, 187, 391, 391, 391, 140, 140, 289, 140, 320, 159, 159, 159, 159, 285, 34, 253, 253, 253, 253, 253, 253, 453, 9, 168, 168, 118, 118, 118, 118, 118, 402, 29, 340, 340, 340, 340, 33, 478, 478, 162, 232, 232, 238, 238, 6, 272, 371, 470, 470, 443, 443, 240, 240, 285, 34, 69, 223, 223, 130, 130, 402, 196, 196, 217, 217, 217, 473, 473, 429, 429, 429, 429, 429, 246, 246, 246, 246, 246, 246, 246, 19, 454, 454, 454, 414, 414, 414, 414, 187, 187, 47, 47, 47, 140, 80, 140, 373, 373, 451, 451, 451, 30, 30, 30, 422, 422, 186, 162, 232, 232, 172, 115, 115, 273, 106, 106, 405, 405, 405, 206, 206, 293, 43, 43, 364, 364, 181, 181, 181, 181, 181, 181, 325, 183, 183, 183, 451, 30, 30, 30, 30, 422, 422, 162, 232, 232, 482, 105, 105, 336, 354, 106, 106, 496, 496, 496, 496, 274, 274, 35, 35, 259, 192, 180, 223, 223, 223, 223, 223, 223, 223, 223, 223, 173, 352, 352, 352, 352, 427, 414, 414]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 405, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_121-127105-0002
generate
processing 16th semantic_sys file
16
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: THERE WAS SOMETHING INDIVIDUAL ABOUT THE GREAT FARM A MOST UNUSUAL TRIMNESS AND CARE FOR DETAIL
enroll_x_lens:tensor([43], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
277
[17, 17, 296, 114, 0, 222, 378, 345, 141, 141, 141, 281, 162, 232, 232, 172, 115, 231, 231, 231, 231, 53, 76, 76, 465, 214, 214, 214, 214, 214, 200, 200, 464, 121, 121, 116, 33, 394, 212, 239, 384, 490, 490, 490, 4, 4, 280, 280, 278, 278, 278, 278, 278, 24, 310, 310, 107, 395, 395, 134, 134, 134, 100, 100, 100, 100, 100, 100, 375, 497, 175, 175, 255, 255, 255, 8, 8, 354, 180, 113, 113, 113, 113, 113, 167, 35, 401, 401, 82, 22, 283, 455, 416, 416, 239, 144, 79, 79, 484, 484, 484, 484, 484, 484, 252, 457, 457, 90, 393, 205, 205, 261, 25, 106, 284, 306, 306, 306, 306, 396, 396, 203, 53, 53, 70, 44, 44, 399, 399, 70, 70, 65, 496, 496, 274, 186, 39, 86, 238, 6, 272, 483, 440, 319, 319, 319, 348, 10, 219, 219, 219, 485, 485, 374, 374, 374, 368, 368, 107, 107, 395, 134, 134, 100, 100, 100, 497, 122, 129, 36, 161, 161, 161, 487, 487, 288, 288, 278, 203, 53, 53, 10, 10, 479, 459, 459, 459, 459, 271, 271, 39, 54, 390, 390, 390, 18, 18, 112, 427, 56, 56, 56, 56, 56, 56, 47, 20, 20, 47, 20, 20, 47, 20, 20, 20, 20, 73, 73, 20, 20, 83, 55, 446, 67, 33, 90, 90, 458, 445, 445, 351, 351, 264, 468, 468, 468, 245, 245, 349, 155, 155, 332, 332, 332, 313, 236, 239, 239, 384, 371, 213, 213, 213, 252, 143, 36, 108, 119, 119, 351, 470, 470, 264, 264, 139, 139, 139, 139, 375, 375, 98, 98, 98, 13, 13, 414, 414]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 275, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0015
generate
processing 17th semantic_sys file
17
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: HOW BROWN YOU'VE GOT SINCE YOU CAME HOME I WISH I HAD AN ATHLETE TO MOW MY ORCHARD
enroll_x_lens:tensor([41], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
242
[17, 296, 296, 268,[17, 17, 296, 296, 188, 188, 475, 330, 475, 94, 475, 475, 324, 301, 378, 43, 364, 276, 174, 174, 174, 174, 174, 174, 282, 388, 388, 195, 195, 195, 117, 117, 117, 117, 197, 197, 197, 197, 197, 7, 127, 127, 114, 0, 222, 222, 406, 467, 255, 255, 255, 8, 354, 180, 180, 113, 113, 113, 113, 450, 450, 167, 167, 77, 270, 323, 142, 397, 397, 345, 389, 389, 389, 285, 202, 202, 202, 402, 402, 259, 108synthesize text: ANY ONE THEREABOUTS WOULD HAVE TOLD YOU THAT THIS WAS ONE OF THE RICHEST FARMS ON THE DIVIDE AND THAT THE FARMER WAS A WOMAN ALEXANDRA BERGSON
enroll_x_lens:tensor([43], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
588
[17, 296, 296, 188, 475, 475, 475, 94, 475, 475, 475, 301, 378, 43, 364, 174, 174, 174, 174, 174, 348, 348, 195, 195, 195, 195, 195, 195, 195, 117, 117, 117, 117, 197, 197, 197, 197, 197, 197, 197, 197, 197, 7, 127, 127, 127, 127, 114, 114, 264, 264, 264, 468, 406, 406, 467, 467, 255, 255, 8, 8, 354, 180, 180, 113, 113, 113, 113, 113, 450, 450, 167, 167, 233, 270, 270, 270, 390, 390, 390, 390, 18, 97, 97, 225, 225, 225, 225, 225, 225, 7, 7, 7, 345, 345, 389, 389, 389, 389, 285, 34, 202, 202, 202, 202, 402, 221, 401, 401, 82, 108, 119, 119, 351, 106, 424, 424, 424, 424, 424, 424, 182, 497, 497, 122, 122, 24, 310, 107, 477, 477, 477, 477, 477, 477, 477, 132, 132, 132, 132, 132, 98, 98, 13, 229, 247, 247, 126, 126, 292, 292, 292, 292, 292, 292, 292, 292, 292, 292, 292, 21,  30%|██▉       | 19/64 [04:24<10:39, 14.21s/it]2024-03-25 02:32:12 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:12 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
[17, 296, 268, 268, 268, 268, 293, 274, 8, 32, 32, 354, 190, 380, 380, 380, 315, 315, 315, 450, 450, 413, 348, 64, 219, 219, 152, 152, 152, 152, 173, 402, 221, 259, 144, 27, 106, 189, 405, 167, 167, 457, 478, 478, 232, 232, 172, 115, 273, 278, 330, 379, 64, 77, 310, 107, 107, 152, 152, 152, 152, 143, 259, 144, 445, 210, 210, 210, 210, 210, 210, 203, 53, 53, 58, 72, 72, 437, 350, 350, 350, 350, 350, 203, 53, 70, 65, 111, 111, 111, 111, 378, 43, 364, 345, 109, 109, 278, 99, 99, 436, 107, 395, 106, 111, 111, 111, 438, 438, 58, 58, 110, 110, 254, 254, 254, 240, 314, 314, 196, 242, 116, 94, 199, 44, 44, 44, 38, 164, 164, 164, 164, 26, 26, 251, 241, 444, 444, 213, 213, 252, 457, 401, 82, 140, 108, 377, 377, 374, 374, 374, 132, 399, 217, 70, 65, 65, 84, 496, 496, 274, 274, 399, 70, 70, 46, 46, 46, 46, 46, 464, 106, 106, 106, 153, 387, 387, 387, 387, 167, 233, 310, 107, 107, 395, 395, 334, 334, 37, 37, 24, 131, 419, 439, 78, 140, 140]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 195, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0011
generate
processing 18th semantic_sys file
18
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: ANY ONE THEREABOUTS WOULD HAVE TOLD YOU THAT THIS WAS ONE OF THE RICHEST FARMS ON THE DIVIDE AND THAT THE FARMER WAS A WOMAN ALEXANDRA BERGSON
enroll_x_lens:tensor([43], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
567
[17, 17, 17, 296, 188, 188, 475, 475, 475, 475, 475, 475, 475, 475, 324, 324, 301, 378, 43, 364, 276, 174, 174, 174, 174, 174, 174, 282, 388, 348, 195, 195, 466, 466, 127, 114, 114, 264, 264, 264, 468, 468, 467, 467, 255, 255, 255, 8, 8, 354, 180, 180, 486, 113, 113, 113, 113, 450, 450, 450, 413, 167, 77, 270, 270, 323, 323, 142, 397, 397, 345, 345, 389, 389, 389, 285, 202, 202, 202, 402, 129, 82, 75, 108, 119, 351, 106, 424, 424, 424, 424, 424, 424, 424, 497, 122, 122, 122, 310, 107, 477, 477, 477, 477, 477, 477, 132, 132, 132, 216, 216, 127, 45, 45, 45, 45, 45, 45, 35, 35, 401, 401, 82, 127, 114, 114, 258, 258, 258, 258, 258, 258, 271, 271, 271, 39, 54, 54, 54, 390, 390, 18, 18, 97, 427, 56, 56, 247, 187, 101, 149, 149, 228, 82, 82, 320, 345, 141, 141, 141, 141, 281, 453, 9, 397, 133, 364, 276, 276, 174, 174, 174, 174, 174, 348, 199, 223, 223, 223, 216, 216, 22, 283, 283, 455, 455, 42, 42, 42, 147, 147, 380, 288, 278, 278, 278, 143, 310, 107, 107, 395, 459, 459, 271, 31, 54, 238, 6, 6, 472, 393, 393, 234, 261, 261, 25, 106, 284, 284, 284, 284, 306, 306, 306, 306, 306, 306, 306, 306, 396, 396, 203, 203, 53, 53, 471, 49, 49, 453, 168, 168, 125, 125, 125, 125, 125, 348, 466, 22, 283, 283, 455, 236, 239, 490, 490, 490, 490, 4, 4, 280, 280, 106, 106, 265, 265, 265, 265, 265, 265, 265, 265, 265, 85, 85, 85, 85, 85, 85, 207, 207, 37, 37, 24, 131, 404, 439, 439, 439, 78, 78, 170, 170, 140, 28, 140, 28, 140, 28, 28, 28, 140, 140, 341, 341, 341, 341, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 369, 260, 163, 163, 163, 163, 163, 140, 140, 316, 316, 316, 140, 73, 73, 140, 412, 83, 83, 83, 194, 194, 194, 194, 194, 194, 282, 388, 67, 64, 212, 212, 127, 45, 45, 45, 45, 45, 45, 167, 167, 457, 35, 401, 401, 82, 127, 22, 5, 5, 455, 349, 349, 205, 234, 261, 25, 25, 106, 284, 306, 306, 306, 306, 306, 396, 396, 203, 53, 29, 334, 334, 334, 59, 59, 59, 452, 452, 263, 263, 263, 225, 225, 225, 225, 225, 80, 80, 82, 7, 7, 345, 345, 141, 141, 141, 141, 281, 453, 9, 168, 44, 44, 44, 44, 43, 364, 276, 276, 276, 174, 174, 174, 174, 399, 53, 53, 473, 242, 242, 116, 94, 199, 145, 145, 145, 145, 486, 486, 139, 139, 175, 175, 175, 81, 81, 81, 459, 207, 207, 37, 37, 233, 233, 233, 270, 270, 270, 390, 390, 18, 18, 18, 112, 439, 439, 78, 78, 170, 140, 28, 28, 140, 28, 28, 2, 2, 2, 2, 140, 341, 341, 341, 369, 369, 369, 369, 369, 369, 369, 369, 369, 260, 260, 260, 391, 149, 228, 228, 140, 412, 83, 83, 55, 55, 55, 322, 67, 67, 212, 42, 147, 147, 380, 380, 499, 496, 496, 496, 496, 274, 274, 416, 416, 96, 96, 270, 323, 323, 224, 275, 275, 275, 275, 303, 303, 303, 48, 48, 48, 417]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 565, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0017
generate
processing 19th semantic_sys file
19
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: HIS WIFE NOW LIES BESIDE HIM AND THE WHITE SHAFT THAT MARKS THEIR GRAVES GLEAMS ACROSS THE WHEAT FIELDS
enroll_x_lens:tensor([29], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
489
[17, 17, 296, 451, 257, 257, 257, 281, 453, 9, 142, 133, 364, 276, 276, 346, 346, 346, 428, 428, 428, 146, 146, 358, 358, 358, 349, 352, 352, 352, 352, 352, 352, 97, 97, 225, 225, 7, 309, 479, 479, 331, 315, 315, 450, 293, 293, 293, 251, 251, 251, 241, 431, 431, 265, 265, 265, 265, 85, 85, 146, 146, 318, 368, 453, 9, 142, 221, 336, 354, 420, 420, 422, 422, 162, 232, 172, 115, 273, 273, 265, 265, 428, 146, 146, 252, 252, 325, 183, 183, 57, 57, 57, 57, 203, 381, 381, 381, 117, 117, 48, 417, 417, 417, 417, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 2 36%|███▌      | 23/64 [04:46<08:48, 12.90s/it]2024-03-25 02:32:34 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:34 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
18, 218, 218, 218, 218, 218, 218, 218, 218, 20, 20, 218, 218, 218, 20, 218, 218, 218, 218, 20, 218, 218, 20, 218, 218, 218, 218, 218, 218, 20, 366, 366, 20, 20, 366, 366, 366, 366, 366, 366, 366, 366, 366, 20, 366, 366, 366, 20, 316, 316, 316, 316, 316, 316, 73, 20, 289, 412, 412, 83, 55, 322, 322, 466, 466, 22, 5, 455, 43, 43, 364, 276, 276, 346, 346, 428, 428, 146, 146, 252, 143, 36, 310, 310, 338, 338, 338, 395, 395, 470, 470, 486, 486, 486, 460, 460, 460, 169, 169, 352, 402, 96, 401, 401, 198, 127, 45, 45, 45, 45, 385, 314, 196, 196, 217, 70, 70, 65, 65, [17, 17, 127, 5, 448, 448, 448, 448, 3, 3, 3, 14, 14, 411, 411, 145, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 468, 468, 59, 59, 59, 59, 452, 335, 440, 440, 83, 89, 89, 446, 446, 67, 466, 466, 22, 22, 448, 448, 448, 448, 3, 14, 14, 411, 411, 498, 498, 498, 498, 498, 498, 498, 498, 498, 396, 169, 169, 169, 164, 164, 164, 164, 164, 164, 164, 164, 164, 97, 483, 226, 226, 20, 20, 287, 353, 353, 353, 353, 353, 353, 396, 396, 245, 143, 35, 458, 445, 445, 445, 485, 485, 485, 485, 485, 485, 374, 468, 468, 468, 468, 337, 337, 337, 337, 459, 459, 459, 459, 271, 31, 39, 54, 54, 86, 26, 26, 359, 359, 166, 166, 166, 166, 166, 301, 399, 217, 217, 217, 217, 473, 473, 65, 476, 476, 476, 171, 171, 171, 252, 252, 252, 325, 34, 191, 191, 191, 191, 191, 37, 37, 37, 24, 24, 24, 404, 404, 404, 404, 404, 439, 78, 229, 20, 247, 312, 312, 126, 126, 292, 292, 292, 292, 292, 292, 292, 292, 292, 21, 21, 21, 21, 21, 408, 408, 408, 408, 408, 149, 228, 20, 20, 20, 412, 83, 83, 55, 55, 55, 322, 67, 67, 212, 212, 131, 335, 188, 188, 121, 121, 121, 33, 394, 76, 465, 108, 119, 119, 351, [17, 296, 127, 5, 448, 448, 448, 3, 14, 14, 411, 411, 264, 264, 264, 264, 264, 264, 264, 468, 468, 468, 59, 467, 335, 440, 89, 89, 446, 446, 466, 466, 466, 22, 448, 448, 448, 3, 14, 411, 411, 498, 498, 498, 498, 498, 396, 169, 169, 169, 164, 164, 164, 164, 483, 440, 353, 353, 353, 353, 245, 245,  33%|███▎      | 21/64 [04:48<09:07, 12.73s/it]2024-03-25 02:32:36 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:36 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
 34%|███▍      | 22/64 [04:54<07:30, 10.72s/it]2024-03-25 02:32:42 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:42 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
 36%|███▌      | 23/64 [04:56<05:31,  8.10s/it]2024-03-25 02:32:44 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:44 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 487, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0001
generate
processing 20th semantic_sys file
20
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: IT IS SIXTEEN YEARS SINCE JOHN BERGSON DIED
enroll_x_lens:tensor([41], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
204
[17, 17, 296, 287, 287, 111, 111, 111, 438, 438, 438, 143, 36, 36, 449, 371, 444, 213, 213, 213, 246, 246, 246, 246, 246, 246, 3, 3, 3, 3, 3, 197, 226, 20, 20, 20, 20, 209, 188, 188, 356, 356, 356, 356, 281, 31, 342, 342, 232, 232, 68, 172, 115, 273, 273, 278, 178, 143, 96, 96, 96, 86, 86, 86, 238, 6, 6, 272, 119, 119, 360, 360, 360, 360, 360, 434, 434, 434, 339, 339, 33, 33, 219, 219, 219, 286, 286, 286, 286, 286, 286, 286, 286, 286, 286, 355, 355, 50, 50, 185, 185, 185, 49, 269, 433, 433, 160, 97, 97, 225, 225, 225, 225, 225, 373, 66, 66, 68, 172, 115, 273, 273, 278, 330, 379, 379, 77, 478, 478, 54, 482, 238, 6, 6, 310, 395, 329, 329, 426, 426, 426, 348, 348, 64, 64, 212, 212, 354, 180, 180, 498, 498, 498, 498, 498, 178, 178, 458, 96, 270, 342, 342, 224, 224, 242, 116, 116, 33, 33, 394, 212, 239, 384, 371, 180, 180, 265, 265, 265, 265, 265, 265, 265, 85, 85, 85, 299, 207, 207, 207, 37, 37, 24, 24, 131, 227, 419, 419, 419, 439, 439, 439, 439, 439]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 202, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0000
generate
processing 21th semantic_sys file
21
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: THE AIR AND THE EARTH ARE CURIOUSLY MATED AND INTERMINGLED AS IF THE ONE WERE THE BREATH OF THE OTHER
enroll_x_lens:tensor([41], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
355
[17, 296, 127, 5, 5, 448, 448, 14, 14, 411, 411, 264, 264, 264, 264, 264, 264, 264, 468, 468, 468, 467, 335, 335, 440, 440, 89, 446, 446, 446, 466, 466, 22, 448, 448, 448, 3, 14, 14, 411, 498, 498, 498, 498, 498, 498, 396, 396, 169, 169, 164, 164, 483, 483, 440, 353, 353, 353, 353, 396, 245, 143, 458, 458, 445, 445, 485, 485, 485, 485, 468, 468, 468, 468, 337, 337, 459, 459, 459, 459, 31, 39, 342, 86, 26, 26, 359, 166, 166, 166, 166, 166, 301, 301, 399, 217, 217, 217, 473, 473, 65, 476, 476, 171, 171, 171, 252, 252, 252, 325, 325, 34, 191, 191, 191, 191, 191, 37, 37, 24, 131, 131, 404, 404, 404, 225, 225, 225, 226, 140, 140, 140, 209, 83, 83, 55, 55, 446, 322, 67, 212, 131, 34, 188, 121, 121, 121, 33, 394, 76, 465, 108, 119, 119, 308, 308, 308, 308, 308, 203, 53, 473, 176, 176, 135, 328, 200, 200, 248, 212, 144, 180, 106, 106, 91, 91, 91, 481, 182, 375, 375, 375, 122, 122, 24, 131, 404, 404, 439, 78, 140, 247, 312, 126, 292, 292, 326, 326, 326, 408, 408, 149, 149, 228, 140, 140, 412, 83, 83, 253, 253, 253, 253, 253, 453, 453, 168, 118, 118, 118, 118, 118, 402, 402, 198, 198, 22, 5, 5, 455, 455, 43, 43, 364, 364, 276, 276, 276, 276, 174, 174, 174, 174, 174, 174, 174, 282, 282, 282, 282, 388, 195, 195, 195, 404, 404, 404, 404, 404, 225, 197, 197, 7, 7, 364, 345, 347, 347, 347, 313, 216, 216, 216, 283, 283, 455, 455, 8, 32, 32, 32, 32, 32, 354, 190, 190, 380, 380, 380, 380, 380, 288, 443, 443, 443, 443, 120, 169, 169, 150, 150, 164, 164, 164, 164, 164, 164, 164, 164, 97, 483, 226, 226, 140, 140, 440, 69, 69, 69, 223, 130, 130, 198, 198, 22, 283, 448, 448, 448, 464, 464, 464, 180, 493, 493, 493, 493, 493, 216, 216, 300, 300, 300, 334, 334, 59, 59, 59, 59, 452, 452, 452, 263, 263, 263]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 353, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0004
generate
processing 22th semantic_sys file
22
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: INDEED HE HAD LOOKED AWAY WITH THE PURPOSE OF NOT SEEING IT
enroll_x_lens:tensor([38], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
259
[17, 296, 188, 188, 121, 121, 121, 121, 33, 33, 394, 212, 239, 384, 371, 371, 444, 213, 213, 213, 213, 213, 213, 246, 246, 246, 246, 246, 246, 246, 252, 24, 24, 131, 404, 183, 183, 183, 183, 451, 451, 451, 30, 30, 30, 30, 30, 30, 3, 3, 58, 58, 110, 110, 254, 254, 254, 254, 254, 314, 314, 26, 251, 251, 251, 241, 367, 367, 367, 367, 367, 35, 96, 96, 401, 82, 272, 34, 255, 255, 255, 43, 43, 364, 364, 276, 276, 109, 109, 403, 403, 403, 403, 403, 403, 403, 207, 207, 207, 207, 207, 207, 207, 207, 207, 19, 3, 301, 133, 364, 364, 345, 333, 333, 220, 220, 220, 164, 164, 22, 283, 283, 455, 455, 129, 129, 82, 74, 74, 492, 492, 492, 492, 492, 492, 396, 215, 215, 35, 35, 259, 29, 459, 459, 459, 271, 31, 39, 54, 54, 390, 390, 390, 390, 18, 97, 483, 226, 226, 226, 226, 140, 209, 287, 69, 69, 223, 223, 130, 130, 402, 402, 196, 309, 309, 309, 479, 331, 331, 307, 307, 307, 307, 307, 61, 61, 167, 167, 457, 478, 478, 66, 232, 232, 172, 172, 115, 267, 267, 267, 267, 267, 267, 267, 464, 464, 176, 135, 135, 135, 200, 200, 200, 248, 335, 14, 14, 411, 287, 284, 265, 265, 265, 265, 428, 85, 146, 146, 146, 252, 252, 143, 129, 401, 82, 108, 108, 119, 119, 351, 213, 213, 213, 213, 213, 213, 246, 246, 246, 246, 246, 246, 246, 246, 246, 19, 19, 454, 454]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 257, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0013
generate
processing 23th semantic_sys file
23
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: THERE IS EVEN A WHITE ROW OF BEEHIVES IN THE ORCHARD UNDER THE WALNUT TREES
enroll_x_lens:tensor([39], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
222
 38%|███▊      | 24/64 [04:58<04:05,  6.15s/it]2024-03-25 02:32:46 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:46 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
 39%|███▉      | 25/64 [05:00<03:10,  4.89s/it]2024-03-25 02:32:48 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:48 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
 41%|████      | 26/64 [05:02<02:33,  4.03s/it]2024-03-25 02:32:50 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:50 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
[17, 17, 296, 114, 0, 0, 222, 468, 406, 406, 467, 356, 356, 356, 356, 356, 281, 342, 342, 342, 483, 483, 411, 411, 357, 357, 357, 357, 357, 173, 280, 29, 242, 242, 116, 199, 44, 44, 44, 43, 43, 364, 364, 276, 276, 346, 346, 346, 428, 428, 146, 146, 252, 457, 457, 133, 42, 42, 42, 147, 147, 147, 380, 380, 499, 499, 84, 84, 496, 16, 274, 274, 88, 88, 69, 462, 462, 462, 402, 402, 401, 140, 321, 75, 354, 420, 420, 324, 324, 3, 58, 58, 72, 72, 437, 284, 284, 265, 265, 265, 85, 85, 85, 85, 85, 299, 299, 173, 173, 270, 270, 342, 342, 168, 89, 340, 116, 466, 466, 22, 283, 455, 455, 14, 411, 411, 153, 153, 153, 387, 372, 372, 396, 396, 143, 36, 75, 310, 107, 107, 395, 334, 334, 334, 59, 59, 59, 37, 24, 131, 131, 483, 226, 140, 140, 209, 287, 319, 319, 348, 67, 212, 300, 300, 494, 216, 216, 22, 283, 455, 455, 43, 364, 364, 276, 276, 346, 346, 481, 481, 481, 293, 497, 497, 497, 122, 399, 479, 331, 331, 319, 319, 167, 457, 457, 401, 140, 75, 75, 161, 161, 487, 487, 487, 288, 213, 213, 213, 246, 246, 246, 318, 185, 269, 433, 433, 160, 160, 112, 427, 140, 140]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 220, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0018
generate
processing 24th semantic_sys file
24
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: THAT'S NOT MUCH OF A JOB FOR AN ATHLETE HERE I'VE BEEN TO TOWN AND BACK
enroll_x_lens:tensor([44], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
272
[17, 17, 127, 114, 92, 92, 92, 92, 92, 240, 35, 77, 342, 86, 86, 221, 196, 309, 309, 309, 479, 331, 307, 307, 307, 307, 307, 61, 167, 167, 457, 457, 457, 196, 217, 70, 383, 383, 383, 383, 383, 383, 35, 36, 310, 107, 395, 69, 223, 223, 130, 280, 44, 44, 44, 236, 239, 239, 310, 107, 395, 395, 180, 106, 499, 284, 284, 405, 405, 405, 405, 206, 206, 206, 215, 215, 35, 35, 401, 401, 401, 20, 20, 20, 20, 197, 197, 393, 155, 155, 155, 155, 332, 332, 332, 332, 332, 372, 467, 44, 44, 44, 44, 44, 94, 199, 335, 14, 411, 411, 145, 145, 145, 145, 486, 486, 486, 460, 460, 169, 169, 35, 35, 164, 164, 26, 359, 359, 81, 81, 81, 324, 324, 324, 252, 325, 325, 183, 183, 183, 183, 451, 286, 286, 286, 286, 286, 286, 286, 286, 468, 59, 59, 59, 59, 452, 452, 263, 229, 247, 247, 126, 126, 126, 326, 326, 326, 326, 326, 101, 101, 101, 149, 149, 228, 228, 289, 20, 20, 287, 287, 111, 111, 111, 111, 438, 438, 202, 202, 202, 402, 402, 401, 259, 354, 137, 137, 137, 137, 137, 137, 33, 33, 394, 394, 76, 465, 108, 108, 377, 377, 344, 344, 374, 374, 374, 132, 132, 422, 143, 36, 108, 119, 119, 351, 351, 315, 315, 315, 315, 315, 315, 450, 450, 450, 413, 413, 94, 199, 89, 89, 89, 446, 446, 33, 33, 394, 212, 239, 354, 180, 180, 376, 376, 376, 376, 376, 376, 376, 376, 376, 376, 460, 178, 178, 233, 20, 20, 192, 192, 419, 419]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 270, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134493-0006
generate
processing 25th semantic_sys file
25
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: OH EVER SO MUCH ONLY HE SEEMS KIND OF STAID AND SCHOOL TEACHERY
enroll_x_lens:tensor([54], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
278
[17, 17, 296, 287, 287, 16, 16, 16, 16, 16, 16, 16, 16, 16, 274, 274, 88, 335, 14, 411, 145, 463, 463, 463, 463, 463, 280, 29, 29, 382, 382, 313, 313, 186, 162, 162, 232, 232, 172, 172, 115, 344, 344, 344, 344, 344, 344, 274, 274, 399, 399, 70, 70, 383, 383, 383, 383, 383, 383, 383, 35, 310, 107, 107, 447, 483, 14, 14, 411, 350, 350, 350, 350, 350, 350, 350, 250, 250, 250, 359, 81, 166, 166, 166, 324, 3, 183, 183, 451, 451, 30, 30, 30, 422, 422, 162, 162, 232, 172, 115, 444, 444, 444, 360, 434, 203, 53, 71, 71, 49, 9, 142, 221, 336, 82, 144, 27, 27, 437, 437, 480, 480, 480, 480, 480, 480, 85, 85, 299, 299, 299, 299, 339, 339, 64, 64, 212, 131, 34, 462, 462, 462, 130, 402, 478, 162, 232, 232, 482, 238, 238, 336, 82, 384, 470, 470, 470, 403, 403, 171, 171, 171, 171, 246, 246, 246, 246, 246, 246, 37, 37, 24, 131, 404, 404, 404, 439, 439, 78, 78, 140, 140, 140, 312, 312, 312, 187, 12, 12, 12, 12, 23, 23, 23, 23, 23, 23, 23, 23, 260, 260, 260, 391, 391, 228, 140, 412, 412, 83, 83, 55, 55, 446, 446, 67, 33, 394, 478, 478, 162, 232, 482, 105, 105, 336, 336, 208, 153, 153, 153, 153, 153, 182, 182, 182, 497, 497, 497, 122, 122, 129, 82, 108, 108, 119, 351, 213, 213, 213, 213, 252, 143, 36, 310, 107, 107, 395, 395, 495, 495, 495, 406, 406, 467, 337, 41, 41, 41, 41, 41, 19, 19, 19, 19, 19, 454, 454, 454]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
torch.Size([1, 276, 16])
output_dir is /home/v-zhijunjia/data/tts_test_librispeech/nar_test/converted_can_del_tts/baseline/update_all_nar_mask_t_0_nar_mask_r_0.5_gp_i_mask_True_knowtoken_topk_update/base-line-no-prompt-tts-2stages_topk_stage1_2_topk_stage2_70_top_k_know_token_stage2_10_steps_16_2024-03-25_10:27:26
sys_file:gen_237-134500-0021
generate
processing 26th semantic_sys file
26
args.target_mode==1 or args.target_mode==2
semantic nums is 1
synthesize text: I CAN'T PLAY WITH YOU LIKE A LITTLE BOY ANY MORE HE SAID SLOWLY THAT'S WHAT YOU MISS MARIE
enroll_x_lens:tensor([38], dtype=torch.int32)
txt2semantic need not prompt
before_semantic:
after is :
270
[17, 17, 296, 287, 111, 111, 438, 143, 458, 144, 27, 351, 389, 389, 319, 319, 319, 167, 457, 76, 401, 401, 20, 74, 425, 425, 386, 386, 431, 403, 403, 403, 403, 403, 207, 207, 207, 301, 43, 345, 333, 333, 333, 220, 220, 164, 164, 219, 219, 477, 477, 477, 132, 132, 26, 251, 241, 266, 266, 266, 266, 178, 458, 192, 44, 44, 44, 251, 241, 431, 278, 278, 278, 449, 302, 302, 497, 497, 8, 8, 354, 354, 153, 153, 153, 153, 153, 153, 387, 387, 387, 387, 207, 207, 207, 207, 19, 454, 454, 229, 20, 247, 126, 126, 292, 292, 292, 23, 23, 23, 408, 408, 149, 149, 228, 20, 20, 412, 83, 83, 475, 475, 475, 475, 475, 475, 475, 301, 399, 217, 70, 138, 138, 138, 138, 138, 138, 138, 182, 387, 387, 58, 58, 451, 30, 30, 30, 422, 162, 232, 172, 115, 470, 470, 120, 240, 240, 314, 478, 478, 232, 232, 482, 26, 26, 26, 241, 431, 431, 84, 496, 496, 274, 252, 359, 359, 474, 474, 474, 474, 19, 19, 454, 229, 229, 20, 247, 312, 126, 126, 292, 23, 23, 23, 101, 101, 149, 149, 228, 20, 20, 20, 320, 127, 114, 92, 92, 92, 92, 167, 167, 77, 54, 54, 142, 397, 181, 181, 181, 181, 181, 457, 219, 219, 152, 152, 152, 399, 399, 473, 473, 258, 258, 258, 31, 54, 142, 221, 196, 217, 473, 65, 65, 495, 42, 42, 147, 380, 380, 288, 288, 403, 403, 403, 403, 207, 207, 207, 207, 19, 19, 19, 454, 454, 454, 439, 439, 439, 237, 237]
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10
top_k_know_token:10 42%|████▏     | 27/64 [05:04<02:06,  3.41s/it]2024-03-25 02:32:52 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
2024-03-25 02:32:52 | WARNING | phonemizer | words count mismatch on 100.0% of the lines (1/1)
